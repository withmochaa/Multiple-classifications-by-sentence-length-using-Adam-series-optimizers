{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.024495</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>-0.026438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052471</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>0.070235</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.019260</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>-0.111884</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>0.081292</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>-0.017665</td>\n",
       "      <td>-0.066712</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.028377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0.052890</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>-0.026194</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029386</td>\n",
       "      <td>-0.031209</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>-0.037120</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>-0.018892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.010776</td>\n",
       "      <td>-0.059786</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.051542</td>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.028524</td>\n",
       "      <td>-0.073995</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.044442</td>\n",
       "      <td>-0.011845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.061647</td>\n",
       "      <td>-0.012255</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>-0.074443</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.061543</td>\n",
       "      <td>-0.030229</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>0.031141</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>-0.031109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0.053431</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.020079</td>\n",
       "      <td>-0.014064</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>-0.022649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>0.012878</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.053713</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>-0.028455</td>\n",
       "      <td>-0.068289</td>\n",
       "      <td>-0.023170</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>-0.005654</td>\n",
       "      <td>-0.020707</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.065275</td>\n",
       "      <td>-0.021353</td>\n",
       "      <td>-0.016826</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>-0.014433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044845</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.061416</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.027930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.057368</td>\n",
       "      <td>-0.067020</td>\n",
       "      <td>-0.021661</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>-0.005443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>-0.007638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>0.092511</td>\n",
       "      <td>-0.087806</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.014545</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.038338</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018739</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.053627</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "5194  0.060464 -0.050530 -0.024495 -0.003713  0.007245 -0.015306  0.008505   \n",
       "2909  0.070235 -0.000670 -0.005877  0.019260  0.049900  0.023832 -0.111884   \n",
       "4508  0.052890 -0.046060 -0.026194  0.001728  0.007006 -0.002986 -0.004742   \n",
       "2565  0.010776 -0.059786 -0.004754  0.006551  0.052498 -0.009808 -0.051542   \n",
       "1266  0.061647 -0.012255 -0.016091  0.029144  0.056603  0.009113 -0.074443   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4192  0.053431 -0.052661 -0.020079 -0.014064  0.044447  0.010956 -0.080566   \n",
       "9376  0.012878 -0.002010 -0.005473  0.016296  0.029421 -0.009452 -0.053713   \n",
       "1558  0.060729 -0.065275 -0.021353 -0.016826  0.015080 -0.005845  0.017968   \n",
       "9268  0.057368 -0.067020 -0.021661 -0.008400  0.037588  0.018623 -0.096125   \n",
       "9202  0.092511 -0.087806 -0.008856 -0.009114  0.021612 -0.014545 -0.007849   \n",
       "\n",
       "             7         8         9  ...       759       760       761  \\\n",
       "5194 -0.005648  0.080134 -0.026438  ... -0.052471  0.006276  0.017020   \n",
       "2909  0.028072  0.081292  0.012795  ... -0.005734  0.024869 -0.017665   \n",
       "4508  0.012338  0.046945 -0.000430  ... -0.029386 -0.031209  0.026094   \n",
       "2565  0.021771  0.030049 -0.006876  ... -0.020808 -0.011432 -0.036248   \n",
       "1266  0.020431  0.065041 -0.010261  ... -0.005012  0.031044 -0.010521   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4192  0.013217  0.053437 -0.004254  ... -0.033098  0.016860  0.034480   \n",
       "9376 -0.000222  0.026502 -0.029793  ... -0.028686 -0.044635 -0.028455   \n",
       "1558 -0.016174  0.051739 -0.014433  ... -0.044845  0.002285  0.020310   \n",
       "9268  0.030758  0.038464 -0.005443  ... -0.020493  0.001956  0.034335   \n",
       "9202  0.014917  0.038338 -0.009828  ... -0.018739 -0.001494  0.000943   \n",
       "\n",
       "           762       763       764       765       766       767  predict  \n",
       "5194 -0.030626 -0.023270 -0.015483  0.049698  0.008954 -0.017806        2  \n",
       "2909 -0.066712 -0.032936 -0.004115  0.013544  0.016949 -0.028377        0  \n",
       "4508 -0.037120 -0.015718  0.007099  0.023380  0.012774 -0.018892        1  \n",
       "2565 -0.028524 -0.073995  0.019307  0.017825  0.044442 -0.011845        0  \n",
       "1266 -0.061543 -0.030229 -0.018880  0.031141  0.023403 -0.031109        1  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "4192  0.000815 -0.002638  0.023958 -0.002627  0.018100 -0.022649        0  \n",
       "9376 -0.068289 -0.023170  0.004450  0.031209 -0.005654 -0.020707        2  \n",
       "1558 -0.049782 -0.061416  0.011900  0.019114  0.024721 -0.027930        0  \n",
       "9268 -0.003283 -0.027212  0.016575 -0.003495  0.034584 -0.007638        0  \n",
       "9202 -0.051091 -0.053627  0.016019  0.027715 -0.010810 -0.013578        0  \n",
       "\n",
       "[10244 rows x 769 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_row=pd.read_csv(\"/Users/withmocha/Desktop/DATA/Capston Design(2024)/data(sentence)/0426/train/sentence 4/train_data_after_vector.csv\",index_col=0)\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.DataFrame(data=(data_row['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predict\n",
       "5194        2\n",
       "2909        0\n",
       "4508        1\n",
       "2565        0\n",
       "1266        1\n",
       "...       ...\n",
       "4192        0\n",
       "9376        2\n",
       "1558        0\n",
       "9268        0\n",
       "9202        0\n",
       "\n",
       "[10244 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row.drop(columns=['predict'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.024495</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>-0.026438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.052471</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.017806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>0.070235</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.019260</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>-0.111884</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>0.081292</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>-0.017665</td>\n",
       "      <td>-0.066712</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.028377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0.052890</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>-0.026194</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024466</td>\n",
       "      <td>-0.029386</td>\n",
       "      <td>-0.031209</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>-0.037120</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>-0.018892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.010776</td>\n",
       "      <td>-0.059786</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.051542</td>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055455</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.028524</td>\n",
       "      <td>-0.073995</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.044442</td>\n",
       "      <td>-0.011845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.061647</td>\n",
       "      <td>-0.012255</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>-0.074443</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.061543</td>\n",
       "      <td>-0.030229</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>0.031141</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>-0.031109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0.053431</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.020079</td>\n",
       "      <td>-0.014064</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029488</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>-0.022649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>0.012878</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.053713</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024747</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>-0.028455</td>\n",
       "      <td>-0.068289</td>\n",
       "      <td>-0.023170</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>-0.005654</td>\n",
       "      <td>-0.020707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.065275</td>\n",
       "      <td>-0.021353</td>\n",
       "      <td>-0.016826</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>-0.014433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029676</td>\n",
       "      <td>-0.044845</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.061416</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.027930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.057368</td>\n",
       "      <td>-0.067020</td>\n",
       "      <td>-0.021661</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>-0.005443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022327</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>-0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>0.092511</td>\n",
       "      <td>-0.087806</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.014545</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.038338</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074324</td>\n",
       "      <td>-0.018739</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.053627</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.013578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "5194  0.060464 -0.050530 -0.024495 -0.003713  0.007245 -0.015306  0.008505   \n",
       "2909  0.070235 -0.000670 -0.005877  0.019260  0.049900  0.023832 -0.111884   \n",
       "4508  0.052890 -0.046060 -0.026194  0.001728  0.007006 -0.002986 -0.004742   \n",
       "2565  0.010776 -0.059786 -0.004754  0.006551  0.052498 -0.009808 -0.051542   \n",
       "1266  0.061647 -0.012255 -0.016091  0.029144  0.056603  0.009113 -0.074443   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4192  0.053431 -0.052661 -0.020079 -0.014064  0.044447  0.010956 -0.080566   \n",
       "9376  0.012878 -0.002010 -0.005473  0.016296  0.029421 -0.009452 -0.053713   \n",
       "1558  0.060729 -0.065275 -0.021353 -0.016826  0.015080 -0.005845  0.017968   \n",
       "9268  0.057368 -0.067020 -0.021661 -0.008400  0.037588  0.018623 -0.096125   \n",
       "9202  0.092511 -0.087806 -0.008856 -0.009114  0.021612 -0.014545 -0.007849   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "5194 -0.005648  0.080134 -0.026438  ...  0.013090 -0.052471  0.006276   \n",
       "2909  0.028072  0.081292  0.012795  ... -0.011476 -0.005734  0.024869   \n",
       "4508  0.012338  0.046945 -0.000430  ... -0.024466 -0.029386 -0.031209   \n",
       "2565  0.021771  0.030049 -0.006876  ... -0.055455 -0.020808 -0.011432   \n",
       "1266  0.020431  0.065041 -0.010261  ... -0.019674 -0.005012  0.031044   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4192  0.013217  0.053437 -0.004254  ... -0.029488 -0.033098  0.016860   \n",
       "9376 -0.000222  0.026502 -0.029793  ... -0.024747 -0.028686 -0.044635   \n",
       "1558 -0.016174  0.051739 -0.014433  ... -0.029676 -0.044845  0.002285   \n",
       "9268  0.030758  0.038464 -0.005443  ... -0.022327 -0.020493  0.001956   \n",
       "9202  0.014917  0.038338 -0.009828  ... -0.074324 -0.018739 -0.001494   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "5194  0.017020 -0.030626 -0.023270 -0.015483  0.049698  0.008954 -0.017806  \n",
       "2909 -0.017665 -0.066712 -0.032936 -0.004115  0.013544  0.016949 -0.028377  \n",
       "4508  0.026094 -0.037120 -0.015718  0.007099  0.023380  0.012774 -0.018892  \n",
       "2565 -0.036248 -0.028524 -0.073995  0.019307  0.017825  0.044442 -0.011845  \n",
       "1266 -0.010521 -0.061543 -0.030229 -0.018880  0.031141  0.023403 -0.031109  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4192  0.034480  0.000815 -0.002638  0.023958 -0.002627  0.018100 -0.022649  \n",
       "9376 -0.028455 -0.068289 -0.023170  0.004450  0.031209 -0.005654 -0.020707  \n",
       "1558  0.020310 -0.049782 -0.061416  0.011900  0.019114  0.024721 -0.027930  \n",
       "9268  0.034335 -0.003283 -0.027212  0.016575 -0.003495  0.034584 -0.007638  \n",
       "9202  0.000943 -0.051091 -0.053627  0.016019  0.027715 -0.010810 -0.013578  \n",
       "\n",
       "[10244 rows x 768 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.DataFrame(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.024495</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>-0.026438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.052471</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.017806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>0.070235</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.019260</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>-0.111884</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>0.081292</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>-0.017665</td>\n",
       "      <td>-0.066712</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.028377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0.052890</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>-0.026194</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024466</td>\n",
       "      <td>-0.029386</td>\n",
       "      <td>-0.031209</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>-0.037120</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>-0.018892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.010776</td>\n",
       "      <td>-0.059786</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.051542</td>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055455</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.028524</td>\n",
       "      <td>-0.073995</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.044442</td>\n",
       "      <td>-0.011845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.061647</td>\n",
       "      <td>-0.012255</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>-0.074443</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.061543</td>\n",
       "      <td>-0.030229</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>0.031141</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>-0.031109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0.053431</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.020079</td>\n",
       "      <td>-0.014064</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029488</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>-0.022649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>0.012878</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.053713</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024747</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>-0.028455</td>\n",
       "      <td>-0.068289</td>\n",
       "      <td>-0.023170</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>-0.005654</td>\n",
       "      <td>-0.020707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.065275</td>\n",
       "      <td>-0.021353</td>\n",
       "      <td>-0.016826</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>-0.014433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029676</td>\n",
       "      <td>-0.044845</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.061416</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.027930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.057368</td>\n",
       "      <td>-0.067020</td>\n",
       "      <td>-0.021661</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>-0.005443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022327</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>-0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>0.092511</td>\n",
       "      <td>-0.087806</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.014545</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.038338</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074324</td>\n",
       "      <td>-0.018739</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.053627</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.013578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "5194  0.060464 -0.050530 -0.024495 -0.003713  0.007245 -0.015306  0.008505   \n",
       "2909  0.070235 -0.000670 -0.005877  0.019260  0.049900  0.023832 -0.111884   \n",
       "4508  0.052890 -0.046060 -0.026194  0.001728  0.007006 -0.002986 -0.004742   \n",
       "2565  0.010776 -0.059786 -0.004754  0.006551  0.052498 -0.009808 -0.051542   \n",
       "1266  0.061647 -0.012255 -0.016091  0.029144  0.056603  0.009113 -0.074443   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4192  0.053431 -0.052661 -0.020079 -0.014064  0.044447  0.010956 -0.080566   \n",
       "9376  0.012878 -0.002010 -0.005473  0.016296  0.029421 -0.009452 -0.053713   \n",
       "1558  0.060729 -0.065275 -0.021353 -0.016826  0.015080 -0.005845  0.017968   \n",
       "9268  0.057368 -0.067020 -0.021661 -0.008400  0.037588  0.018623 -0.096125   \n",
       "9202  0.092511 -0.087806 -0.008856 -0.009114  0.021612 -0.014545 -0.007849   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "5194 -0.005648  0.080134 -0.026438  ...  0.013090 -0.052471  0.006276   \n",
       "2909  0.028072  0.081292  0.012795  ... -0.011476 -0.005734  0.024869   \n",
       "4508  0.012338  0.046945 -0.000430  ... -0.024466 -0.029386 -0.031209   \n",
       "2565  0.021771  0.030049 -0.006876  ... -0.055455 -0.020808 -0.011432   \n",
       "1266  0.020431  0.065041 -0.010261  ... -0.019674 -0.005012  0.031044   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4192  0.013217  0.053437 -0.004254  ... -0.029488 -0.033098  0.016860   \n",
       "9376 -0.000222  0.026502 -0.029793  ... -0.024747 -0.028686 -0.044635   \n",
       "1558 -0.016174  0.051739 -0.014433  ... -0.029676 -0.044845  0.002285   \n",
       "9268  0.030758  0.038464 -0.005443  ... -0.022327 -0.020493  0.001956   \n",
       "9202  0.014917  0.038338 -0.009828  ... -0.074324 -0.018739 -0.001494   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "5194  0.017020 -0.030626 -0.023270 -0.015483  0.049698  0.008954 -0.017806  \n",
       "2909 -0.017665 -0.066712 -0.032936 -0.004115  0.013544  0.016949 -0.028377  \n",
       "4508  0.026094 -0.037120 -0.015718  0.007099  0.023380  0.012774 -0.018892  \n",
       "2565 -0.036248 -0.028524 -0.073995  0.019307  0.017825  0.044442 -0.011845  \n",
       "1266 -0.010521 -0.061543 -0.030229 -0.018880  0.031141  0.023403 -0.031109  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4192  0.034480  0.000815 -0.002638  0.023958 -0.002627  0.018100 -0.022649  \n",
       "9376 -0.028455 -0.068289 -0.023170  0.004450  0.031209 -0.005654 -0.020707  \n",
       "1558  0.020310 -0.049782 -0.061416  0.011900  0.019114  0.024721 -0.027930  \n",
       "9268  0.034335 -0.003283 -0.027212  0.016575 -0.003495  0.034584 -0.007638  \n",
       "9202  0.000943 -0.051091 -0.053627  0.016019  0.027715 -0.010810 -0.013578  \n",
       "\n",
       "[10244 rows x 768 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10244, 768)\n",
      "(10244, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"/Users/withmocha/Desktop/DATA/Capston Design(2024)/data(sentence)/0426/test/sentence 4/test_data_after_vector.csv\",index_col=0)\n",
    "\n",
    "test_y=pd.DataFrame(data=test['predict'],columns=['predict'])\n",
    "test_x=test.drop(columns=['predict'])\n",
    "test_x=test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.044851</td>\n",
       "      <td>-0.042430</td>\n",
       "      <td>-0.006961</td>\n",
       "      <td>-0.013484</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>-0.001879</td>\n",
       "      <td>-0.029217</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.056450</td>\n",
       "      <td>-0.029752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009869</td>\n",
       "      <td>-0.037630</td>\n",
       "      <td>-0.031069</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.048754</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.031324</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>-0.030652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.053685</td>\n",
       "      <td>-0.031713</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>0.035611</td>\n",
       "      <td>0.027481</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>-0.049530</td>\n",
       "      <td>0.054116</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>-0.014978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>-0.013205</td>\n",
       "      <td>0.022555</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>-0.041614</td>\n",
       "      <td>-0.062523</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.025832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.037837</td>\n",
       "      <td>-0.038545</td>\n",
       "      <td>-0.014687</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>-0.054969</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>0.048658</td>\n",
       "      <td>-0.009814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031416</td>\n",
       "      <td>-0.024065</td>\n",
       "      <td>-0.045052</td>\n",
       "      <td>0.006396</td>\n",
       "      <td>-0.043735</td>\n",
       "      <td>-0.008737</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>-0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.056282</td>\n",
       "      <td>-0.021455</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>-0.005888</td>\n",
       "      <td>0.030137</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.091849</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.089135</td>\n",
       "      <td>-0.019116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025137</td>\n",
       "      <td>-0.037159</td>\n",
       "      <td>0.025403</td>\n",
       "      <td>-0.011662</td>\n",
       "      <td>-0.043762</td>\n",
       "      <td>-0.031354</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.033255</td>\n",
       "      <td>0.036421</td>\n",
       "      <td>-0.013153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.058817</td>\n",
       "      <td>0.038383</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.058204</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.023911</td>\n",
       "      <td>-0.021514</td>\n",
       "      <td>0.065904</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>0.027966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022382</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>0.016855</td>\n",
       "      <td>-0.059979</td>\n",
       "      <td>-0.051967</td>\n",
       "      <td>-0.031533</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>-0.039962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.097472</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>-0.023503</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.008652</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.062014</td>\n",
       "      <td>-0.026497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030102</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>-0.040595</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>-0.028417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.045834</td>\n",
       "      <td>-0.064345</td>\n",
       "      <td>-0.031244</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>-0.011592</td>\n",
       "      <td>-0.003974</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.038335</td>\n",
       "      <td>-0.006581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035320</td>\n",
       "      <td>-0.024150</td>\n",
       "      <td>-0.026688</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.039419</td>\n",
       "      <td>-0.053324</td>\n",
       "      <td>0.032473</td>\n",
       "      <td>0.022905</td>\n",
       "      <td>-0.006476</td>\n",
       "      <td>-0.023545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.040264</td>\n",
       "      <td>-0.073166</td>\n",
       "      <td>-0.025342</td>\n",
       "      <td>-0.017914</td>\n",
       "      <td>0.032022</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>-0.048681</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>0.060075</td>\n",
       "      <td>-0.030883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030425</td>\n",
       "      <td>-0.029200</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>-0.010937</td>\n",
       "      <td>-0.022380</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>-0.004861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.032927</td>\n",
       "      <td>-0.010039</td>\n",
       "      <td>-0.009479</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.031543</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.034750</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>-0.011754</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065038</td>\n",
       "      <td>-0.027960</td>\n",
       "      <td>-0.048410</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>-0.021921</td>\n",
       "      <td>0.049584</td>\n",
       "      <td>-0.020645</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>-0.023911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.054210</td>\n",
       "      <td>-0.071686</td>\n",
       "      <td>-0.029723</td>\n",
       "      <td>-0.016055</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>-0.003630</td>\n",
       "      <td>-0.018938</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>-0.026981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007215</td>\n",
       "      <td>-0.013544</td>\n",
       "      <td>-0.029878</td>\n",
       "      <td>0.024908</td>\n",
       "      <td>-0.043535</td>\n",
       "      <td>-0.052556</td>\n",
       "      <td>-0.011865</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.027771</td>\n",
       "      <td>-0.012426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "167  0.044851 -0.042430 -0.006961 -0.013484  0.002387 -0.001879 -0.029217   \n",
       "285  0.053685 -0.031713 -0.005566  0.035611  0.027481  0.009243 -0.049530   \n",
       "283  0.037837 -0.038545 -0.014687  0.011569  0.045468  0.006111 -0.054969   \n",
       "203  0.056282 -0.021455 -0.007533 -0.005888  0.030137  0.009732 -0.091849   \n",
       "289  0.058817  0.038383  0.006154  0.058204  0.021400  0.023911 -0.021514   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "174  0.097472 -0.028992 -0.023503  0.006292  0.010449  0.009685 -0.008652   \n",
       "47   0.045834 -0.064345 -0.031244 -0.004216 -0.011592 -0.003974  0.012940   \n",
       "104  0.040264 -0.073166 -0.025342 -0.017914  0.032022  0.016343 -0.048681   \n",
       "251  0.032927 -0.010039 -0.009479  0.005613  0.031543 -0.004185 -0.034750   \n",
       "246  0.054210 -0.071686 -0.029723 -0.016055  0.026487 -0.003630 -0.018938   \n",
       "\n",
       "            7         8         9  ...       758       759       760  \\\n",
       "167  0.034203  0.056450 -0.029752  ... -0.009869 -0.037630 -0.031069   \n",
       "285  0.054116  0.065669 -0.014978  ... -0.023383 -0.013205  0.022555   \n",
       "283  0.035764  0.048658 -0.009814  ... -0.031416 -0.024065 -0.045052   \n",
       "203  0.032324  0.089135 -0.019116  ... -0.025137 -0.037159  0.025403   \n",
       "289  0.065904  0.048862  0.027966  ... -0.022382  0.002063  0.038624   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "174 -0.000601  0.062014 -0.026497  ... -0.030102 -0.003908 -0.005393   \n",
       "47   0.008767  0.038335 -0.006581  ... -0.035320 -0.024150 -0.026688   \n",
       "104  0.038254  0.060075 -0.030883  ... -0.030425 -0.029200 -0.001801   \n",
       "251  0.078247 -0.011754  0.020567  ... -0.065038 -0.027960 -0.048410   \n",
       "246  0.000601  0.041702 -0.026981  ... -0.007215 -0.013544 -0.029878   \n",
       "\n",
       "          761       762       763       764       765       766       767  \n",
       "167 -0.009059 -0.048754 -0.065758  0.012232  0.031324  0.014519 -0.030652  \n",
       "285  0.001222 -0.041614 -0.062523  0.012539  0.017402  0.000446 -0.025832  \n",
       "283  0.006396 -0.043735 -0.008737  0.007495  0.001713  0.006389 -0.026621  \n",
       "203 -0.011662 -0.043762 -0.031354  0.018539  0.033255  0.036421 -0.013153  \n",
       "289  0.016855 -0.059979 -0.051967 -0.031533  0.002043  0.005134 -0.039962  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "174  0.027009 -0.040595 -0.053467  0.017463  0.019107  0.010120 -0.028417  \n",
       "47   0.009888 -0.039419 -0.053324  0.032473  0.022905 -0.006476 -0.023545  \n",
       "104  0.019856 -0.010937 -0.022380  0.010038  0.014976  0.010054 -0.004861  \n",
       "251  0.017365 -0.021921  0.049584 -0.020645  0.018671  0.014556 -0.023911  \n",
       "246  0.024908 -0.043535 -0.052556 -0.011865  0.004778  0.027771 -0.012426  \n",
       "\n",
       "[300 rows x 768 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predict\n",
       "167        0\n",
       "285        2\n",
       "283        0\n",
       "203        0\n",
       "289        0\n",
       "..       ...\n",
       "174        1\n",
       "47         2\n",
       "104        1\n",
       "251        0\n",
       "246        1\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#x_train = scaler.fit_transform(x_train)\n",
    "#x_test = scaler.transform(x_test) # test set에는 transform만 사용하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>0.060464</td>\n",
       "      <td>-0.050530</td>\n",
       "      <td>-0.024495</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>-0.015306</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>0.080134</td>\n",
       "      <td>-0.026438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.052471</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.017806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>0.070235</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.019260</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>-0.111884</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>0.081292</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011476</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>-0.017665</td>\n",
       "      <td>-0.066712</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.028377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0.052890</td>\n",
       "      <td>-0.046060</td>\n",
       "      <td>-0.026194</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>-0.000430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024466</td>\n",
       "      <td>-0.029386</td>\n",
       "      <td>-0.031209</td>\n",
       "      <td>0.026094</td>\n",
       "      <td>-0.037120</td>\n",
       "      <td>-0.015718</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>-0.018892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.010776</td>\n",
       "      <td>-0.059786</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>-0.009808</td>\n",
       "      <td>-0.051542</td>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.030049</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055455</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>-0.028524</td>\n",
       "      <td>-0.073995</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>0.044442</td>\n",
       "      <td>-0.011845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.061647</td>\n",
       "      <td>-0.012255</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.056603</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>-0.074443</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>-0.061543</td>\n",
       "      <td>-0.030229</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>0.031141</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>-0.031109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>0.053431</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.020079</td>\n",
       "      <td>-0.014064</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029488</td>\n",
       "      <td>-0.033098</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>-0.022649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>0.012878</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>-0.053713</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024747</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>-0.028455</td>\n",
       "      <td>-0.068289</td>\n",
       "      <td>-0.023170</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.031209</td>\n",
       "      <td>-0.005654</td>\n",
       "      <td>-0.020707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.060729</td>\n",
       "      <td>-0.065275</td>\n",
       "      <td>-0.021353</td>\n",
       "      <td>-0.016826</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>-0.014433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029676</td>\n",
       "      <td>-0.044845</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.061416</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.027930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.057368</td>\n",
       "      <td>-0.067020</td>\n",
       "      <td>-0.021661</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>-0.005443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022327</td>\n",
       "      <td>-0.020493</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>-0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9202</th>\n",
       "      <td>0.092511</td>\n",
       "      <td>-0.087806</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>-0.014545</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.038338</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074324</td>\n",
       "      <td>-0.018739</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.053627</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.013578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "5194  0.060464 -0.050530 -0.024495 -0.003713  0.007245 -0.015306  0.008505   \n",
       "2909  0.070235 -0.000670 -0.005877  0.019260  0.049900  0.023832 -0.111884   \n",
       "4508  0.052890 -0.046060 -0.026194  0.001728  0.007006 -0.002986 -0.004742   \n",
       "2565  0.010776 -0.059786 -0.004754  0.006551  0.052498 -0.009808 -0.051542   \n",
       "1266  0.061647 -0.012255 -0.016091  0.029144  0.056603  0.009113 -0.074443   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4192  0.053431 -0.052661 -0.020079 -0.014064  0.044447  0.010956 -0.080566   \n",
       "9376  0.012878 -0.002010 -0.005473  0.016296  0.029421 -0.009452 -0.053713   \n",
       "1558  0.060729 -0.065275 -0.021353 -0.016826  0.015080 -0.005845  0.017968   \n",
       "9268  0.057368 -0.067020 -0.021661 -0.008400  0.037588  0.018623 -0.096125   \n",
       "9202  0.092511 -0.087806 -0.008856 -0.009114  0.021612 -0.014545 -0.007849   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "5194 -0.005648  0.080134 -0.026438  ...  0.013090 -0.052471  0.006276   \n",
       "2909  0.028072  0.081292  0.012795  ... -0.011476 -0.005734  0.024869   \n",
       "4508  0.012338  0.046945 -0.000430  ... -0.024466 -0.029386 -0.031209   \n",
       "2565  0.021771  0.030049 -0.006876  ... -0.055455 -0.020808 -0.011432   \n",
       "1266  0.020431  0.065041 -0.010261  ... -0.019674 -0.005012  0.031044   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4192  0.013217  0.053437 -0.004254  ... -0.029488 -0.033098  0.016860   \n",
       "9376 -0.000222  0.026502 -0.029793  ... -0.024747 -0.028686 -0.044635   \n",
       "1558 -0.016174  0.051739 -0.014433  ... -0.029676 -0.044845  0.002285   \n",
       "9268  0.030758  0.038464 -0.005443  ... -0.022327 -0.020493  0.001956   \n",
       "9202  0.014917  0.038338 -0.009828  ... -0.074324 -0.018739 -0.001494   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "5194  0.017020 -0.030626 -0.023270 -0.015483  0.049698  0.008954 -0.017806  \n",
       "2909 -0.017665 -0.066712 -0.032936 -0.004115  0.013544  0.016949 -0.028377  \n",
       "4508  0.026094 -0.037120 -0.015718  0.007099  0.023380  0.012774 -0.018892  \n",
       "2565 -0.036248 -0.028524 -0.073995  0.019307  0.017825  0.044442 -0.011845  \n",
       "1266 -0.010521 -0.061543 -0.030229 -0.018880  0.031141  0.023403 -0.031109  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4192  0.034480  0.000815 -0.002638  0.023958 -0.002627  0.018100 -0.022649  \n",
       "9376 -0.028455 -0.068289 -0.023170  0.004450  0.031209 -0.005654 -0.020707  \n",
       "1558  0.020310 -0.049782 -0.061416  0.011900  0.019114  0.024721 -0.027930  \n",
       "9268  0.034335 -0.003283 -0.027212  0.016575 -0.003495  0.034584 -0.007638  \n",
       "9202  0.000943 -0.051091 -0.053627  0.016019  0.027715 -0.010810 -0.013578  \n",
       "\n",
       "[10244 rows x 768 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train=to_categorical(y_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 차원 정보 : (10244, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "test_x = tf.reshape(test_x,(test_x.shape[0],test_x.shape[1],1))\n",
    "print('변경된 차원 정보 :',x_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10244, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10244, 768, 1), dtype=float64, numpy=\n",
       "array([[[ 0.06046394],\n",
       "        [-0.05053037],\n",
       "        [-0.02449477],\n",
       "        ...,\n",
       "        [ 0.04969777],\n",
       "        [ 0.00895411],\n",
       "        [-0.01780626]],\n",
       "\n",
       "       [[ 0.07023466],\n",
       "        [-0.00066958],\n",
       "        [-0.00587679],\n",
       "        ...,\n",
       "        [ 0.01354356],\n",
       "        [ 0.01694879],\n",
       "        [-0.02837737]],\n",
       "\n",
       "       [[ 0.05288995],\n",
       "        [-0.04605973],\n",
       "        [-0.02619425],\n",
       "        ...,\n",
       "        [ 0.02337986],\n",
       "        [ 0.01277419],\n",
       "        [-0.01889212]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.06072892],\n",
       "        [-0.06527454],\n",
       "        [-0.02135302],\n",
       "        ...,\n",
       "        [ 0.01911428],\n",
       "        [ 0.02472117],\n",
       "        [-0.02793025]],\n",
       "\n",
       "       [[ 0.05736763],\n",
       "        [-0.06701954],\n",
       "        [-0.0216611 ],\n",
       "        ...,\n",
       "        [-0.00349525],\n",
       "        [ 0.03458394],\n",
       "        [-0.00763782]],\n",
       "\n",
       "       [[ 0.09251116],\n",
       "        [-0.08780576],\n",
       "        [-0.00885551],\n",
       "        ...,\n",
       "        [ 0.02771536],\n",
       "        [-0.01080968],\n",
       "        [-0.01357779]]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# ReduceLROnPlateau 콜백 생성\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', # 모니터링 대상 성능 지표\n",
    "                              factor=0.95,        # 학습률 감소 비율\n",
    "                              patience=10,        # 성능 향상을 기다리는 에폭 수\n",
    "                              verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">766</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">762</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">381</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">381</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">379</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">377</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m766\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m762\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m381\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m381\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m379\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m1,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m377\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m41,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m41,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m10,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,795</span> (413.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,795\u001b[0m (413.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,795</span> (413.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,795\u001b[0m (413.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.activations import elu\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# 입력층 - 1D CNN\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='elu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='elu'))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "# 추가 CNN 층\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='elu'))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "# 중간층 - LSTM\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "# 추가 Dense 층\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 출력층\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 최적화 알고리즘과 손실 함수 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.002,weight_decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping과 Model Checkpoint 적용\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True,verbose=1,mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_weights.keras', monitor='val_accuracy',save_best_only=True,verbose=1,mode='max',save_weights_only=False)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "\n",
    "start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4931 - loss: 1.0265\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55490, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.4947 - loss: 1.0254 - val_accuracy: 0.5549 - val_loss: 0.9627 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871ms/step - accuracy: 0.5545 - loss: 0.9766\n",
      "Epoch 2: val_accuracy did not improve from 0.55490\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 943ms/step - accuracy: 0.5548 - loss: 0.9764 - val_accuracy: 0.5549 - val_loss: 0.9614 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881ms/step - accuracy: 0.5615 - loss: 0.9678\n",
      "Epoch 3: val_accuracy did not improve from 0.55490\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 954ms/step - accuracy: 0.5617 - loss: 0.9677 - val_accuracy: 0.5549 - val_loss: 0.9590 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866ms/step - accuracy: 0.5696 - loss: 0.9534\n",
      "Epoch 4: val_accuracy improved from 0.55490 to 0.63494, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 947ms/step - accuracy: 0.5695 - loss: 0.9532 - val_accuracy: 0.6349 - val_loss: 0.8547 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995ms/step - accuracy: 0.6201 - loss: 0.8752\n",
      "Epoch 5: val_accuracy improved from 0.63494 to 0.71694, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.6211 - loss: 0.8736 - val_accuracy: 0.7169 - val_loss: 0.6955 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818ms/step - accuracy: 0.6875 - loss: 0.7692\n",
      "Epoch 6: val_accuracy improved from 0.71694 to 0.75500, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 895ms/step - accuracy: 0.6881 - loss: 0.7683 - val_accuracy: 0.7550 - val_loss: 0.5992 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.7218 - loss: 0.7105\n",
      "Epoch 7: val_accuracy did not improve from 0.75500\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 784ms/step - accuracy: 0.7213 - loss: 0.7110 - val_accuracy: 0.6676 - val_loss: 0.7368 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714ms/step - accuracy: 0.6937 - loss: 0.7506\n",
      "Epoch 8: val_accuracy improved from 0.75500 to 0.76379, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 784ms/step - accuracy: 0.6943 - loss: 0.7494 - val_accuracy: 0.7638 - val_loss: 0.5728 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718ms/step - accuracy: 0.7298 - loss: 0.6713\n",
      "Epoch 9: val_accuracy improved from 0.76379 to 0.76672, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 791ms/step - accuracy: 0.7300 - loss: 0.6711 - val_accuracy: 0.7667 - val_loss: 0.5617 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733ms/step - accuracy: 0.7554 - loss: 0.6396\n",
      "Epoch 10: val_accuracy improved from 0.76672 to 0.77501, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 805ms/step - accuracy: 0.7551 - loss: 0.6398 - val_accuracy: 0.7750 - val_loss: 0.5403 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726ms/step - accuracy: 0.7529 - loss: 0.6423\n",
      "Epoch 11: val_accuracy improved from 0.77501 to 0.77696, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 796ms/step - accuracy: 0.7526 - loss: 0.6425 - val_accuracy: 0.7770 - val_loss: 0.5366 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907ms/step - accuracy: 0.7515 - loss: 0.6262\n",
      "Epoch 12: val_accuracy did not improve from 0.77696\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 995ms/step - accuracy: 0.7516 - loss: 0.6259 - val_accuracy: 0.7755 - val_loss: 0.5303 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844ms/step - accuracy: 0.7564 - loss: 0.6200\n",
      "Epoch 13: val_accuracy improved from 0.77696 to 0.78526, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 914ms/step - accuracy: 0.7565 - loss: 0.6195 - val_accuracy: 0.7853 - val_loss: 0.4959 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.7585 - loss: 0.5976\n",
      "Epoch 14: val_accuracy improved from 0.78526 to 0.78673, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 883ms/step - accuracy: 0.7586 - loss: 0.5979 - val_accuracy: 0.7867 - val_loss: 0.5073 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7600 - loss: 0.5908\n",
      "Epoch 15: val_accuracy improved from 0.78673 to 0.78917, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.7601 - loss: 0.5910 - val_accuracy: 0.7892 - val_loss: 0.4895 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7631 - loss: 0.5969\n",
      "Epoch 16: val_accuracy improved from 0.78917 to 0.79746, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.7632 - loss: 0.5961 - val_accuracy: 0.7975 - val_loss: 0.4597 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7633 - loss: 0.5712\n",
      "Epoch 17: val_accuracy did not improve from 0.79746\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7636 - loss: 0.5711 - val_accuracy: 0.7936 - val_loss: 0.4632 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842ms/step - accuracy: 0.7761 - loss: 0.5537\n",
      "Epoch 18: val_accuracy improved from 0.79746 to 0.79844, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 919ms/step - accuracy: 0.7760 - loss: 0.5539 - val_accuracy: 0.7984 - val_loss: 0.4646 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773ms/step - accuracy: 0.7738 - loss: 0.5576\n",
      "Epoch 19: val_accuracy improved from 0.79844 to 0.80137, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 846ms/step - accuracy: 0.7739 - loss: 0.5574 - val_accuracy: 0.8014 - val_loss: 0.4587 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837ms/step - accuracy: 0.7877 - loss: 0.5275\n",
      "Epoch 20: val_accuracy improved from 0.80137 to 0.80478, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 923ms/step - accuracy: 0.7875 - loss: 0.5278 - val_accuracy: 0.8048 - val_loss: 0.4406 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - accuracy: 0.7831 - loss: 0.5380\n",
      "Epoch 21: val_accuracy improved from 0.80478 to 0.81259, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 850ms/step - accuracy: 0.7830 - loss: 0.5382 - val_accuracy: 0.8126 - val_loss: 0.4343 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.7784 - loss: 0.5524\n",
      "Epoch 22: val_accuracy did not improve from 0.81259\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 864ms/step - accuracy: 0.7785 - loss: 0.5523 - val_accuracy: 0.8028 - val_loss: 0.4315 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775ms/step - accuracy: 0.7804 - loss: 0.5417\n",
      "Epoch 23: val_accuracy did not improve from 0.81259\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 857ms/step - accuracy: 0.7805 - loss: 0.5413 - val_accuracy: 0.8028 - val_loss: 0.4303 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757ms/step - accuracy: 0.7955 - loss: 0.4971\n",
      "Epoch 24: val_accuracy did not improve from 0.81259\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 841ms/step - accuracy: 0.7952 - loss: 0.4975 - val_accuracy: 0.7901 - val_loss: 0.4651 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907ms/step - accuracy: 0.7949 - loss: 0.5013\n",
      "Epoch 25: val_accuracy improved from 0.81259 to 0.81942, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.7947 - loss: 0.5018 - val_accuracy: 0.8194 - val_loss: 0.4041 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974ms/step - accuracy: 0.7961 - loss: 0.4975\n",
      "Epoch 26: val_accuracy improved from 0.81942 to 0.82577, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7964 - loss: 0.4969 - val_accuracy: 0.8258 - val_loss: 0.3962 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845ms/step - accuracy: 0.7959 - loss: 0.5024\n",
      "Epoch 27: val_accuracy did not improve from 0.82577\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 933ms/step - accuracy: 0.7958 - loss: 0.5021 - val_accuracy: 0.8258 - val_loss: 0.3939 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.8011 - loss: 0.4938\n",
      "Epoch 28: val_accuracy did not improve from 0.82577\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 899ms/step - accuracy: 0.8011 - loss: 0.4936 - val_accuracy: 0.8199 - val_loss: 0.3922 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830ms/step - accuracy: 0.7985 - loss: 0.4849\n",
      "Epoch 29: val_accuracy did not improve from 0.82577\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 918ms/step - accuracy: 0.7987 - loss: 0.4845 - val_accuracy: 0.8126 - val_loss: 0.4135 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868ms/step - accuracy: 0.8006 - loss: 0.4751\n",
      "Epoch 30: val_accuracy did not improve from 0.82577\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 954ms/step - accuracy: 0.8008 - loss: 0.4749 - val_accuracy: 0.8224 - val_loss: 0.3927 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866ms/step - accuracy: 0.8051 - loss: 0.4780\n",
      "Epoch 31: val_accuracy improved from 0.82577 to 0.82626, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 956ms/step - accuracy: 0.8051 - loss: 0.4779 - val_accuracy: 0.8263 - val_loss: 0.3937 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830ms/step - accuracy: 0.8128 - loss: 0.4703\n",
      "Epoch 32: val_accuracy improved from 0.82626 to 0.84187, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 918ms/step - accuracy: 0.8126 - loss: 0.4703 - val_accuracy: 0.8419 - val_loss: 0.3856 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834ms/step - accuracy: 0.8110 - loss: 0.4672\n",
      "Epoch 33: val_accuracy did not improve from 0.84187\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 921ms/step - accuracy: 0.8107 - loss: 0.4676 - val_accuracy: 0.8263 - val_loss: 0.3977 - learning_rate: 0.0020\n",
      "Epoch 34/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784ms/step - accuracy: 0.8088 - loss: 0.4721\n",
      "Epoch 34: val_accuracy did not improve from 0.84187\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 877ms/step - accuracy: 0.8090 - loss: 0.4714 - val_accuracy: 0.8385 - val_loss: 0.3575 - learning_rate: 0.0020\n",
      "Epoch 35/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831ms/step - accuracy: 0.8059 - loss: 0.4851\n",
      "Epoch 35: val_accuracy did not improve from 0.84187\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 919ms/step - accuracy: 0.8060 - loss: 0.4846 - val_accuracy: 0.8336 - val_loss: 0.3716 - learning_rate: 0.0020\n",
      "Epoch 36/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.8080 - loss: 0.4750\n",
      "Epoch 36: val_accuracy improved from 0.84187 to 0.84773, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8083 - loss: 0.4744 - val_accuracy: 0.8477 - val_loss: 0.3576 - learning_rate: 0.0020\n",
      "Epoch 37/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912ms/step - accuracy: 0.8228 - loss: 0.4347\n",
      "Epoch 37: val_accuracy did not improve from 0.84773\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1000ms/step - accuracy: 0.8225 - loss: 0.4353 - val_accuracy: 0.8409 - val_loss: 0.3734 - learning_rate: 0.0020\n",
      "Epoch 38/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8215 - loss: 0.4433\n",
      "Epoch 38: val_accuracy did not improve from 0.84773\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8214 - loss: 0.4434 - val_accuracy: 0.8433 - val_loss: 0.3543 - learning_rate: 0.0020\n",
      "Epoch 39/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807ms/step - accuracy: 0.8093 - loss: 0.4579\n",
      "Epoch 39: val_accuracy improved from 0.84773 to 0.85456, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 894ms/step - accuracy: 0.8095 - loss: 0.4576 - val_accuracy: 0.8546 - val_loss: 0.3526 - learning_rate: 0.0020\n",
      "Epoch 40/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807ms/step - accuracy: 0.8221 - loss: 0.4422\n",
      "Epoch 40: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 894ms/step - accuracy: 0.8221 - loss: 0.4422 - val_accuracy: 0.8526 - val_loss: 0.3512 - learning_rate: 0.0020\n",
      "Epoch 41/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880ms/step - accuracy: 0.8266 - loss: 0.4230\n",
      "Epoch 41: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 969ms/step - accuracy: 0.8266 - loss: 0.4232 - val_accuracy: 0.8380 - val_loss: 0.3870 - learning_rate: 0.0020\n",
      "Epoch 42/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8097 - loss: 0.4539\n",
      "Epoch 42: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8101 - loss: 0.4533 - val_accuracy: 0.8448 - val_loss: 0.3616 - learning_rate: 0.0020\n",
      "Epoch 43/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8158 - loss: 0.4516\n",
      "Epoch 43: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8161 - loss: 0.4510 - val_accuracy: 0.8389 - val_loss: 0.4010 - learning_rate: 0.0020\n",
      "Epoch 44/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8314 - loss: 0.4200\n",
      "Epoch 44: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8312 - loss: 0.4205 - val_accuracy: 0.8468 - val_loss: 0.3325 - learning_rate: 0.0020\n",
      "Epoch 45/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8223 - loss: 0.4248\n",
      "Epoch 45: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8224 - loss: 0.4247 - val_accuracy: 0.8497 - val_loss: 0.3420 - learning_rate: 0.0020\n",
      "Epoch 46/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995ms/step - accuracy: 0.8294 - loss: 0.4295\n",
      "Epoch 46: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8296 - loss: 0.4289 - val_accuracy: 0.8546 - val_loss: 0.3329 - learning_rate: 0.0020\n",
      "Epoch 47/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981ms/step - accuracy: 0.8451 - loss: 0.4040\n",
      "Epoch 47: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8448 - loss: 0.4044 - val_accuracy: 0.8429 - val_loss: 0.3776 - learning_rate: 0.0020\n",
      "Epoch 48/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906ms/step - accuracy: 0.8306 - loss: 0.4038\n",
      "Epoch 48: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8303 - loss: 0.4043 - val_accuracy: 0.8497 - val_loss: 0.3416 - learning_rate: 0.0020\n",
      "Epoch 49/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873ms/step - accuracy: 0.8345 - loss: 0.4142\n",
      "Epoch 49: val_accuracy did not improve from 0.85456\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 963ms/step - accuracy: 0.8344 - loss: 0.4143 - val_accuracy: 0.8516 - val_loss: 0.3655 - learning_rate: 0.0020\n",
      "Epoch 50/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928ms/step - accuracy: 0.8280 - loss: 0.4101\n",
      "Epoch 50: val_accuracy improved from 0.85456 to 0.86823, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8282 - loss: 0.4099 - val_accuracy: 0.8682 - val_loss: 0.3183 - learning_rate: 0.0020\n",
      "Epoch 51/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8426 - loss: 0.3962\n",
      "Epoch 51: val_accuracy did not improve from 0.86823\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8424 - loss: 0.3965 - val_accuracy: 0.8677 - val_loss: 0.3209 - learning_rate: 0.0020\n",
      "Epoch 52/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856ms/step - accuracy: 0.8379 - loss: 0.3974\n",
      "Epoch 52: val_accuracy did not improve from 0.86823\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 942ms/step - accuracy: 0.8377 - loss: 0.3979 - val_accuracy: 0.8648 - val_loss: 0.3353 - learning_rate: 0.0020\n",
      "Epoch 53/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8433 - loss: 0.3979\n",
      "Epoch 53: val_accuracy did not improve from 0.86823\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8433 - loss: 0.3979 - val_accuracy: 0.8633 - val_loss: 0.3139 - learning_rate: 0.0020\n",
      "Epoch 54/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911ms/step - accuracy: 0.8347 - loss: 0.4040\n",
      "Epoch 54: val_accuracy improved from 0.86823 to 0.87018, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8351 - loss: 0.4035 - val_accuracy: 0.8702 - val_loss: 0.3072 - learning_rate: 0.0020\n",
      "Epoch 55/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838ms/step - accuracy: 0.8530 - loss: 0.3776\n",
      "Epoch 55: val_accuracy improved from 0.87018 to 0.88385, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 932ms/step - accuracy: 0.8527 - loss: 0.3781 - val_accuracy: 0.8838 - val_loss: 0.2853 - learning_rate: 0.0020\n",
      "Epoch 56/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913ms/step - accuracy: 0.8479 - loss: 0.3840\n",
      "Epoch 56: val_accuracy did not improve from 0.88385\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8480 - loss: 0.3839 - val_accuracy: 0.8755 - val_loss: 0.3003 - learning_rate: 0.0020\n",
      "Epoch 57/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8472 - loss: 0.3716\n",
      "Epoch 57: val_accuracy did not improve from 0.88385\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8471 - loss: 0.3720 - val_accuracy: 0.8643 - val_loss: 0.3013 - learning_rate: 0.0020\n",
      "Epoch 58/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8498 - loss: 0.3611\n",
      "Epoch 58: val_accuracy improved from 0.88385 to 0.88824, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8497 - loss: 0.3618 - val_accuracy: 0.8882 - val_loss: 0.2852 - learning_rate: 0.0020\n",
      "Epoch 59/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907ms/step - accuracy: 0.8512 - loss: 0.3739\n",
      "Epoch 59: val_accuracy improved from 0.88824 to 0.89458, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 995ms/step - accuracy: 0.8511 - loss: 0.3739 - val_accuracy: 0.8946 - val_loss: 0.2721 - learning_rate: 0.0020\n",
      "Epoch 60/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835ms/step - accuracy: 0.8501 - loss: 0.3751\n",
      "Epoch 60: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 921ms/step - accuracy: 0.8502 - loss: 0.3749 - val_accuracy: 0.8834 - val_loss: 0.2698 - learning_rate: 0.0020\n",
      "Epoch 61/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8596 - loss: 0.3572\n",
      "Epoch 61: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8594 - loss: 0.3574 - val_accuracy: 0.8897 - val_loss: 0.2730 - learning_rate: 0.0020\n",
      "Epoch 62/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949ms/step - accuracy: 0.8585 - loss: 0.3431\n",
      "Epoch 62: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8583 - loss: 0.3435 - val_accuracy: 0.8907 - val_loss: 0.2815 - learning_rate: 0.0020\n",
      "Epoch 63/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8499 - loss: 0.3675\n",
      "Epoch 63: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8500 - loss: 0.3675 - val_accuracy: 0.8902 - val_loss: 0.2724 - learning_rate: 0.0020\n",
      "Epoch 64/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8568 - loss: 0.3689\n",
      "Epoch 64: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8566 - loss: 0.3689 - val_accuracy: 0.8843 - val_loss: 0.2897 - learning_rate: 0.0020\n",
      "Epoch 65/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915ms/step - accuracy: 0.8512 - loss: 0.3779\n",
      "Epoch 65: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8513 - loss: 0.3775 - val_accuracy: 0.8882 - val_loss: 0.2716 - learning_rate: 0.0020\n",
      "Epoch 66/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882ms/step - accuracy: 0.8535 - loss: 0.3488\n",
      "Epoch 66: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 968ms/step - accuracy: 0.8534 - loss: 0.3496 - val_accuracy: 0.8848 - val_loss: 0.2835 - learning_rate: 0.0020\n",
      "Epoch 67/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932ms/step - accuracy: 0.8669 - loss: 0.3470\n",
      "Epoch 67: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8670 - loss: 0.3471 - val_accuracy: 0.8941 - val_loss: 0.2787 - learning_rate: 0.0020\n",
      "Epoch 68/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8606 - loss: 0.3442\n",
      "Epoch 68: val_accuracy did not improve from 0.89458\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8607 - loss: 0.3442 - val_accuracy: 0.8882 - val_loss: 0.2776 - learning_rate: 0.0020\n",
      "Epoch 69/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8669 - loss: 0.3434\n",
      "Epoch 69: val_accuracy improved from 0.89458 to 0.89653, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8670 - loss: 0.3433 - val_accuracy: 0.8965 - val_loss: 0.2773 - learning_rate: 0.0020\n",
      "Epoch 70/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8633 - loss: 0.3466\n",
      "Epoch 70: val_accuracy improved from 0.89653 to 0.90532, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8633 - loss: 0.3467 - val_accuracy: 0.9053 - val_loss: 0.2453 - learning_rate: 0.0020\n",
      "Epoch 71/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8696 - loss: 0.3446\n",
      "Epoch 71: val_accuracy did not improve from 0.90532\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8695 - loss: 0.3445 - val_accuracy: 0.9043 - val_loss: 0.2622 - learning_rate: 0.0020\n",
      "Epoch 72/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8723 - loss: 0.3380\n",
      "Epoch 72: val_accuracy did not improve from 0.90532\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8722 - loss: 0.3378 - val_accuracy: 0.8956 - val_loss: 0.2571 - learning_rate: 0.0020\n",
      "Epoch 73/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.8725 - loss: 0.3252\n",
      "Epoch 73: val_accuracy did not improve from 0.90532\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8724 - loss: 0.3256 - val_accuracy: 0.8965 - val_loss: 0.2648 - learning_rate: 0.0020\n",
      "Epoch 74/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8741 - loss: 0.3171 \n",
      "Epoch 74: val_accuracy improved from 0.90532 to 0.90874, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8739 - loss: 0.3177 - val_accuracy: 0.9087 - val_loss: 0.2407 - learning_rate: 0.0020\n",
      "Epoch 75/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993ms/step - accuracy: 0.8669 - loss: 0.3445\n",
      "Epoch 75: val_accuracy improved from 0.90874 to 0.91362, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8669 - loss: 0.3443 - val_accuracy: 0.9136 - val_loss: 0.2358 - learning_rate: 0.0020\n",
      "Epoch 76/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.8698 - loss: 0.3418\n",
      "Epoch 76: val_accuracy did not improve from 0.91362\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8700 - loss: 0.3413 - val_accuracy: 0.8975 - val_loss: 0.2496 - learning_rate: 0.0020\n",
      "Epoch 77/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.8730 - loss: 0.3190\n",
      "Epoch 77: val_accuracy did not improve from 0.91362\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 949ms/step - accuracy: 0.8729 - loss: 0.3193 - val_accuracy: 0.9068 - val_loss: 0.2329 - learning_rate: 0.0020\n",
      "Epoch 78/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794ms/step - accuracy: 0.8756 - loss: 0.3205\n",
      "Epoch 78: val_accuracy did not improve from 0.91362\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 875ms/step - accuracy: 0.8757 - loss: 0.3206 - val_accuracy: 0.9068 - val_loss: 0.2371 - learning_rate: 0.0020\n",
      "Epoch 79/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758ms/step - accuracy: 0.8745 - loss: 0.3162\n",
      "Epoch 79: val_accuracy did not improve from 0.91362\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 849ms/step - accuracy: 0.8747 - loss: 0.3161 - val_accuracy: 0.9102 - val_loss: 0.2384 - learning_rate: 0.0020\n",
      "Epoch 80/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815ms/step - accuracy: 0.8794 - loss: 0.3042\n",
      "Epoch 80: val_accuracy improved from 0.91362 to 0.91947, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 904ms/step - accuracy: 0.8793 - loss: 0.3047 - val_accuracy: 0.9195 - val_loss: 0.2188 - learning_rate: 0.0020\n",
      "Epoch 81/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908ms/step - accuracy: 0.8765 - loss: 0.3276\n",
      "Epoch 81: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8764 - loss: 0.3278 - val_accuracy: 0.9009 - val_loss: 0.2557 - learning_rate: 0.0020\n",
      "Epoch 82/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8824 - loss: 0.3078\n",
      "Epoch 82: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.8822 - loss: 0.3081 - val_accuracy: 0.9136 - val_loss: 0.2397 - learning_rate: 0.0020\n",
      "Epoch 83/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933ms/step - accuracy: 0.8788 - loss: 0.3206\n",
      "Epoch 83: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8789 - loss: 0.3205 - val_accuracy: 0.9136 - val_loss: 0.2125 - learning_rate: 0.0020\n",
      "Epoch 84/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8871 - loss: 0.3016\n",
      "Epoch 84: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8872 - loss: 0.3017 - val_accuracy: 0.9107 - val_loss: 0.2434 - learning_rate: 0.0020\n",
      "Epoch 85/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896ms/step - accuracy: 0.8876 - loss: 0.2868\n",
      "Epoch 85: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 998ms/step - accuracy: 0.8875 - loss: 0.2872 - val_accuracy: 0.9078 - val_loss: 0.2301 - learning_rate: 0.0020\n",
      "Epoch 86/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8915 - loss: 0.2935\n",
      "Epoch 86: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8915 - loss: 0.2936 - val_accuracy: 0.9165 - val_loss: 0.2104 - learning_rate: 0.0020\n",
      "Epoch 87/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900ms/step - accuracy: 0.8966 - loss: 0.2858\n",
      "Epoch 87: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 994ms/step - accuracy: 0.8963 - loss: 0.2859 - val_accuracy: 0.9029 - val_loss: 0.2547 - learning_rate: 0.0020\n",
      "Epoch 88/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893ms/step - accuracy: 0.8839 - loss: 0.3086\n",
      "Epoch 88: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 978ms/step - accuracy: 0.8837 - loss: 0.3088 - val_accuracy: 0.8814 - val_loss: 0.3134 - learning_rate: 0.0020\n",
      "Epoch 89/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963ms/step - accuracy: 0.8752 - loss: 0.3321\n",
      "Epoch 89: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8754 - loss: 0.3313 - val_accuracy: 0.8838 - val_loss: 0.2791 - learning_rate: 0.0020\n",
      "Epoch 90/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - accuracy: 0.8838 - loss: 0.3074\n",
      "Epoch 90: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 962ms/step - accuracy: 0.8840 - loss: 0.3071 - val_accuracy: 0.9117 - val_loss: 0.2340 - learning_rate: 0.0020\n",
      "Epoch 91/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8921 - loss: 0.2857\n",
      "Epoch 91: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.8922 - loss: 0.2856 - val_accuracy: 0.9161 - val_loss: 0.2120 - learning_rate: 0.0020\n",
      "Epoch 92/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892ms/step - accuracy: 0.8912 - loss: 0.2707\n",
      "Epoch 92: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 974ms/step - accuracy: 0.8911 - loss: 0.2713 - val_accuracy: 0.8882 - val_loss: 0.2756 - learning_rate: 0.0020\n",
      "Epoch 93/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894ms/step - accuracy: 0.8964 - loss: 0.2858\n",
      "Epoch 93: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 982ms/step - accuracy: 0.8963 - loss: 0.2857 - val_accuracy: 0.9156 - val_loss: 0.2326 - learning_rate: 0.0020\n",
      "Epoch 94/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8914 - loss: 0.2865\n",
      "Epoch 94: val_accuracy did not improve from 0.91947\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8915 - loss: 0.2863 - val_accuracy: 0.9097 - val_loss: 0.2246 - learning_rate: 0.0020\n",
      "Epoch 95/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8920 - loss: 0.2840\n",
      "Epoch 95: val_accuracy improved from 0.91947 to 0.92728, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.8923 - loss: 0.2836 - val_accuracy: 0.9273 - val_loss: 0.1982 - learning_rate: 0.0020\n",
      "Epoch 96/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - accuracy: 0.9032 - loss: 0.2620\n",
      "Epoch 96: val_accuracy did not improve from 0.92728\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9029 - loss: 0.2626 - val_accuracy: 0.9248 - val_loss: 0.2006 - learning_rate: 0.0020\n",
      "Epoch 97/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8992 - loss: 0.2694\n",
      "Epoch 97: val_accuracy did not improve from 0.92728\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8991 - loss: 0.2696 - val_accuracy: 0.9219 - val_loss: 0.2137 - learning_rate: 0.0020\n",
      "Epoch 98/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954ms/step - accuracy: 0.9027 - loss: 0.2601\n",
      "Epoch 98: val_accuracy did not improve from 0.92728\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9028 - loss: 0.2602 - val_accuracy: 0.9209 - val_loss: 0.2239 - learning_rate: 0.0020\n",
      "Epoch 99/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.8993 - loss: 0.2626\n",
      "Epoch 99: val_accuracy improved from 0.92728 to 0.93558, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8994 - loss: 0.2622 - val_accuracy: 0.9356 - val_loss: 0.1948 - learning_rate: 0.0020\n",
      "Epoch 100/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9020 - loss: 0.2783\n",
      "Epoch 100: val_accuracy did not improve from 0.93558\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9020 - loss: 0.2779 - val_accuracy: 0.9268 - val_loss: 0.1896 - learning_rate: 0.0020\n",
      "Epoch 101/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - accuracy: 0.9076 - loss: 0.2547\n",
      "Epoch 101: val_accuracy did not improve from 0.93558\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9073 - loss: 0.2552 - val_accuracy: 0.9278 - val_loss: 0.2021 - learning_rate: 0.0020\n",
      "Epoch 102/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9020 - loss: 0.2678\n",
      "Epoch 102: val_accuracy did not improve from 0.93558\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9021 - loss: 0.2675 - val_accuracy: 0.9327 - val_loss: 0.1901 - learning_rate: 0.0020\n",
      "Epoch 103/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9024 - loss: 0.2682\n",
      "Epoch 103: val_accuracy did not improve from 0.93558\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9023 - loss: 0.2687 - val_accuracy: 0.9239 - val_loss: 0.2045 - learning_rate: 0.0020\n",
      "Epoch 104/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997ms/step - accuracy: 0.9037 - loss: 0.2721\n",
      "Epoch 104: val_accuracy improved from 0.93558 to 0.93802, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9037 - loss: 0.2722 - val_accuracy: 0.9380 - val_loss: 0.1889 - learning_rate: 0.0020\n",
      "Epoch 105/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968ms/step - accuracy: 0.8930 - loss: 0.2839\n",
      "Epoch 105: val_accuracy did not improve from 0.93802\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8934 - loss: 0.2834 - val_accuracy: 0.9341 - val_loss: 0.1982 - learning_rate: 0.0020\n",
      "Epoch 106/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9059 - loss: 0.2523\n",
      "Epoch 106: val_accuracy did not improve from 0.93802\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9060 - loss: 0.2525 - val_accuracy: 0.9356 - val_loss: 0.1823 - learning_rate: 0.0020\n",
      "Epoch 107/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9142 - loss: 0.2515\n",
      "Epoch 107: val_accuracy did not improve from 0.93802\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9140 - loss: 0.2517 - val_accuracy: 0.9209 - val_loss: 0.2024 - learning_rate: 0.0020\n",
      "Epoch 108/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9148 - loss: 0.2580\n",
      "Epoch 108: val_accuracy did not improve from 0.93802\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9147 - loss: 0.2581 - val_accuracy: 0.9253 - val_loss: 0.2026 - learning_rate: 0.0020\n",
      "Epoch 109/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.9086 - loss: 0.2560\n",
      "Epoch 109: val_accuracy did not improve from 0.93802\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9086 - loss: 0.2558 - val_accuracy: 0.9131 - val_loss: 0.2267 - learning_rate: 0.0020\n",
      "Epoch 110/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995ms/step - accuracy: 0.9107 - loss: 0.2469\n",
      "Epoch 110: val_accuracy improved from 0.93802 to 0.94583, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9108 - loss: 0.2466 - val_accuracy: 0.9458 - val_loss: 0.1781 - learning_rate: 0.0020\n",
      "Epoch 111/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9059 - loss: 0.2511\n",
      "Epoch 111: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9058 - loss: 0.2512 - val_accuracy: 0.9151 - val_loss: 0.2159 - learning_rate: 0.0020\n",
      "Epoch 112/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.9119 - loss: 0.2401\n",
      "Epoch 112: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9120 - loss: 0.2402 - val_accuracy: 0.9283 - val_loss: 0.1991 - learning_rate: 0.0020\n",
      "Epoch 113/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900ms/step - accuracy: 0.9144 - loss: 0.2372\n",
      "Epoch 113: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 995ms/step - accuracy: 0.9145 - loss: 0.2370 - val_accuracy: 0.9161 - val_loss: 0.2106 - learning_rate: 0.0020\n",
      "Epoch 114/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - accuracy: 0.9199 - loss: 0.2258\n",
      "Epoch 114: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9198 - loss: 0.2261 - val_accuracy: 0.9385 - val_loss: 0.1795 - learning_rate: 0.0020\n",
      "Epoch 115/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9061 - loss: 0.2520\n",
      "Epoch 115: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9061 - loss: 0.2521 - val_accuracy: 0.9229 - val_loss: 0.2052 - learning_rate: 0.0020\n",
      "Epoch 116/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9159 - loss: 0.2451\n",
      "Epoch 116: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9159 - loss: 0.2450 - val_accuracy: 0.9219 - val_loss: 0.2026 - learning_rate: 0.0020\n",
      "Epoch 117/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802ms/step - accuracy: 0.9095 - loss: 0.2367\n",
      "Epoch 117: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 894ms/step - accuracy: 0.9098 - loss: 0.2365 - val_accuracy: 0.9312 - val_loss: 0.1881 - learning_rate: 0.0020\n",
      "Epoch 118/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830ms/step - accuracy: 0.9214 - loss: 0.2233\n",
      "Epoch 118: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 932ms/step - accuracy: 0.9213 - loss: 0.2236 - val_accuracy: 0.9312 - val_loss: 0.1992 - learning_rate: 0.0020\n",
      "Epoch 119/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9188 - loss: 0.2236\n",
      "Epoch 119: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9188 - loss: 0.2233 - val_accuracy: 0.9146 - val_loss: 0.2177 - learning_rate: 0.0020\n",
      "Epoch 120/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9153 - loss: 0.2398\n",
      "Epoch 120: val_accuracy did not improve from 0.94583\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 0.0019000000902451575.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9153 - loss: 0.2399 - val_accuracy: 0.9302 - val_loss: 0.1895 - learning_rate: 0.0020\n",
      "Epoch 121/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990ms/step - accuracy: 0.9239 - loss: 0.2206\n",
      "Epoch 121: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9239 - loss: 0.2206 - val_accuracy: 0.9375 - val_loss: 0.1742 - learning_rate: 0.0019\n",
      "Epoch 122/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912ms/step - accuracy: 0.9110 - loss: 0.2364\n",
      "Epoch 122: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9112 - loss: 0.2364 - val_accuracy: 0.9361 - val_loss: 0.1743 - learning_rate: 0.0019\n",
      "Epoch 123/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.9165 - loss: 0.2304\n",
      "Epoch 123: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9168 - loss: 0.2299 - val_accuracy: 0.9307 - val_loss: 0.1813 - learning_rate: 0.0019\n",
      "Epoch 124/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822ms/step - accuracy: 0.9214 - loss: 0.2112\n",
      "Epoch 124: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 905ms/step - accuracy: 0.9214 - loss: 0.2117 - val_accuracy: 0.9356 - val_loss: 0.1752 - learning_rate: 0.0019\n",
      "Epoch 125/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787ms/step - accuracy: 0.9221 - loss: 0.2239\n",
      "Epoch 125: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 876ms/step - accuracy: 0.9221 - loss: 0.2238 - val_accuracy: 0.9336 - val_loss: 0.1771 - learning_rate: 0.0019\n",
      "Epoch 126/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980ms/step - accuracy: 0.9213 - loss: 0.2165\n",
      "Epoch 126: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9214 - loss: 0.2164 - val_accuracy: 0.9297 - val_loss: 0.1966 - learning_rate: 0.0019\n",
      "Epoch 127/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.9393 - loss: 0.1906\n",
      "Epoch 127: val_accuracy did not improve from 0.94583\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9388 - loss: 0.1913 - val_accuracy: 0.9370 - val_loss: 0.1785 - learning_rate: 0.0019\n",
      "Epoch 128/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9287 - loss: 0.2072\n",
      "Epoch 128: val_accuracy improved from 0.94583 to 0.94729, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9287 - loss: 0.2072 - val_accuracy: 0.9473 - val_loss: 0.1622 - learning_rate: 0.0019\n",
      "Epoch 129/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966ms/step - accuracy: 0.9217 - loss: 0.2374\n",
      "Epoch 129: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9217 - loss: 0.2369 - val_accuracy: 0.9366 - val_loss: 0.1791 - learning_rate: 0.0019\n",
      "Epoch 130/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895ms/step - accuracy: 0.9252 - loss: 0.2119\n",
      "Epoch 130: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 988ms/step - accuracy: 0.9254 - loss: 0.2117 - val_accuracy: 0.9190 - val_loss: 0.2227 - learning_rate: 0.0019\n",
      "Epoch 131/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.9207 - loss: 0.2208\n",
      "Epoch 131: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9208 - loss: 0.2207 - val_accuracy: 0.9346 - val_loss: 0.1980 - learning_rate: 0.0019\n",
      "Epoch 132/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9235 - loss: 0.2189\n",
      "Epoch 132: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9237 - loss: 0.2184 - val_accuracy: 0.9195 - val_loss: 0.2296 - learning_rate: 0.0019\n",
      "Epoch 133/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805ms/step - accuracy: 0.9262 - loss: 0.2152\n",
      "Epoch 133: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 893ms/step - accuracy: 0.9259 - loss: 0.2155 - val_accuracy: 0.9351 - val_loss: 0.1857 - learning_rate: 0.0019\n",
      "Epoch 134/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.9321 - loss: 0.2045\n",
      "Epoch 134: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 867ms/step - accuracy: 0.9321 - loss: 0.2043 - val_accuracy: 0.9400 - val_loss: 0.1750 - learning_rate: 0.0019\n",
      "Epoch 135/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781ms/step - accuracy: 0.9330 - loss: 0.1962\n",
      "Epoch 135: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 869ms/step - accuracy: 0.9329 - loss: 0.1964 - val_accuracy: 0.9287 - val_loss: 0.1970 - learning_rate: 0.0019\n",
      "Epoch 136/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770ms/step - accuracy: 0.9274 - loss: 0.2035\n",
      "Epoch 136: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 861ms/step - accuracy: 0.9274 - loss: 0.2036 - val_accuracy: 0.9400 - val_loss: 0.1663 - learning_rate: 0.0019\n",
      "Epoch 137/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848ms/step - accuracy: 0.9243 - loss: 0.2112\n",
      "Epoch 137: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 952ms/step - accuracy: 0.9243 - loss: 0.2113 - val_accuracy: 0.9424 - val_loss: 0.1720 - learning_rate: 0.0019\n",
      "Epoch 138/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901ms/step - accuracy: 0.9274 - loss: 0.2091\n",
      "Epoch 138: val_accuracy did not improve from 0.94729\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 0.0018050000304356217.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9273 - loss: 0.2092 - val_accuracy: 0.9219 - val_loss: 0.2031 - learning_rate: 0.0019\n",
      "Epoch 139/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938ms/step - accuracy: 0.9163 - loss: 0.2346\n",
      "Epoch 139: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9169 - loss: 0.2335 - val_accuracy: 0.9439 - val_loss: 0.1668 - learning_rate: 0.0018\n",
      "Epoch 140/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878ms/step - accuracy: 0.9311 - loss: 0.1921\n",
      "Epoch 140: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 977ms/step - accuracy: 0.9313 - loss: 0.1920 - val_accuracy: 0.9361 - val_loss: 0.1751 - learning_rate: 0.0018\n",
      "Epoch 141/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9273 - loss: 0.2042\n",
      "Epoch 141: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9274 - loss: 0.2038 - val_accuracy: 0.9366 - val_loss: 0.1748 - learning_rate: 0.0018\n",
      "Epoch 142/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9282 - loss: 0.2026\n",
      "Epoch 142: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9284 - loss: 0.2023 - val_accuracy: 0.9366 - val_loss: 0.1858 - learning_rate: 0.0018\n",
      "Epoch 143/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.9350 - loss: 0.1894\n",
      "Epoch 143: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 962ms/step - accuracy: 0.9350 - loss: 0.1894 - val_accuracy: 0.9322 - val_loss: 0.1771 - learning_rate: 0.0018\n",
      "Epoch 144/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986ms/step - accuracy: 0.9319 - loss: 0.1973\n",
      "Epoch 144: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9321 - loss: 0.1968 - val_accuracy: 0.9424 - val_loss: 0.1722 - learning_rate: 0.0018\n",
      "Epoch 145/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9399 - loss: 0.1895\n",
      "Epoch 145: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9400 - loss: 0.1889 - val_accuracy: 0.9424 - val_loss: 0.1581 - learning_rate: 0.0018\n",
      "Epoch 146/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923ms/step - accuracy: 0.9405 - loss: 0.1699\n",
      "Epoch 146: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9404 - loss: 0.1700 - val_accuracy: 0.9219 - val_loss: 0.2192 - learning_rate: 0.0018\n",
      "Epoch 147/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9376 - loss: 0.1792\n",
      "Epoch 147: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9376 - loss: 0.1794 - val_accuracy: 0.9327 - val_loss: 0.2060 - learning_rate: 0.0018\n",
      "Epoch 148/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890ms/step - accuracy: 0.9402 - loss: 0.1781\n",
      "Epoch 148: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 981ms/step - accuracy: 0.9400 - loss: 0.1784 - val_accuracy: 0.9380 - val_loss: 0.1670 - learning_rate: 0.0018\n",
      "Epoch 149/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9408 - loss: 0.1795\n",
      "Epoch 149: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9407 - loss: 0.1795 - val_accuracy: 0.9346 - val_loss: 0.1908 - learning_rate: 0.0018\n",
      "Epoch 150/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9339 - loss: 0.1933\n",
      "Epoch 150: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9338 - loss: 0.1934 - val_accuracy: 0.9234 - val_loss: 0.2091 - learning_rate: 0.0018\n",
      "Epoch 151/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885ms/step - accuracy: 0.9248 - loss: 0.2295\n",
      "Epoch 151: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 982ms/step - accuracy: 0.9249 - loss: 0.2288 - val_accuracy: 0.9341 - val_loss: 0.1839 - learning_rate: 0.0018\n",
      "Epoch 152/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - accuracy: 0.9389 - loss: 0.1852\n",
      "Epoch 152: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9390 - loss: 0.1851 - val_accuracy: 0.9439 - val_loss: 0.1627 - learning_rate: 0.0018\n",
      "Epoch 153/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9433 - loss: 0.1670\n",
      "Epoch 153: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9433 - loss: 0.1673 - val_accuracy: 0.9429 - val_loss: 0.1478 - learning_rate: 0.0018\n",
      "Epoch 154/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9477 - loss: 0.1593\n",
      "Epoch 154: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9476 - loss: 0.1595 - val_accuracy: 0.9204 - val_loss: 0.2152 - learning_rate: 0.0018\n",
      "Epoch 155/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948ms/step - accuracy: 0.9400 - loss: 0.1859\n",
      "Epoch 155: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9400 - loss: 0.1856 - val_accuracy: 0.9414 - val_loss: 0.1655 - learning_rate: 0.0018\n",
      "Epoch 156/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914ms/step - accuracy: 0.9397 - loss: 0.1677\n",
      "Epoch 156: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9397 - loss: 0.1679 - val_accuracy: 0.9424 - val_loss: 0.1814 - learning_rate: 0.0018\n",
      "Epoch 157/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9452 - loss: 0.1653\n",
      "Epoch 157: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9453 - loss: 0.1650 - val_accuracy: 0.9385 - val_loss: 0.1652 - learning_rate: 0.0018\n",
      "Epoch 158/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996ms/step - accuracy: 0.9434 - loss: 0.1646\n",
      "Epoch 158: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9434 - loss: 0.1643 - val_accuracy: 0.9370 - val_loss: 0.1861 - learning_rate: 0.0018\n",
      "Epoch 159/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895ms/step - accuracy: 0.9414 - loss: 0.1756\n",
      "Epoch 159: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 982ms/step - accuracy: 0.9412 - loss: 0.1763 - val_accuracy: 0.9370 - val_loss: 0.1850 - learning_rate: 0.0018\n",
      "Epoch 160/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9399 - loss: 0.1750\n",
      "Epoch 160: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9399 - loss: 0.1753 - val_accuracy: 0.9444 - val_loss: 0.1576 - learning_rate: 0.0018\n",
      "Epoch 161/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9438 - loss: 0.1644\n",
      "Epoch 161: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9439 - loss: 0.1644 - val_accuracy: 0.9409 - val_loss: 0.1744 - learning_rate: 0.0018\n",
      "Epoch 162/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921ms/step - accuracy: 0.9467 - loss: 0.1657\n",
      "Epoch 162: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9466 - loss: 0.1657 - val_accuracy: 0.9444 - val_loss: 0.1749 - learning_rate: 0.0018\n",
      "Epoch 163/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928ms/step - accuracy: 0.9422 - loss: 0.1746\n",
      "Epoch 163: val_accuracy did not improve from 0.94729\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 0.0017147500067949293.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9422 - loss: 0.1742 - val_accuracy: 0.9185 - val_loss: 0.2440 - learning_rate: 0.0018\n",
      "Epoch 164/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - accuracy: 0.9373 - loss: 0.1814\n",
      "Epoch 164: val_accuracy did not improve from 0.94729\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 967ms/step - accuracy: 0.9374 - loss: 0.1811 - val_accuracy: 0.9366 - val_loss: 0.1862 - learning_rate: 0.0017\n",
      "Epoch 165/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893ms/step - accuracy: 0.9502 - loss: 0.1536\n",
      "Epoch 165: val_accuracy improved from 0.94729 to 0.94876, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 994ms/step - accuracy: 0.9501 - loss: 0.1536 - val_accuracy: 0.9488 - val_loss: 0.1664 - learning_rate: 0.0017\n",
      "Epoch 166/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.9496 - loss: 0.1539\n",
      "Epoch 166: val_accuracy did not improve from 0.94876\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9495 - loss: 0.1542 - val_accuracy: 0.9483 - val_loss: 0.1520 - learning_rate: 0.0017\n",
      "Epoch 167/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9452 - loss: 0.1555\n",
      "Epoch 167: val_accuracy improved from 0.94876 to 0.95120, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9451 - loss: 0.1560 - val_accuracy: 0.9512 - val_loss: 0.1413 - learning_rate: 0.0017\n",
      "Epoch 168/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9379 - loss: 0.1801\n",
      "Epoch 168: val_accuracy did not improve from 0.95120\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9380 - loss: 0.1801 - val_accuracy: 0.9244 - val_loss: 0.1937 - learning_rate: 0.0017\n",
      "Epoch 169/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864ms/step - accuracy: 0.9434 - loss: 0.1666\n",
      "Epoch 169: val_accuracy did not improve from 0.95120\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 953ms/step - accuracy: 0.9436 - loss: 0.1663 - val_accuracy: 0.9453 - val_loss: 0.1507 - learning_rate: 0.0017\n",
      "Epoch 170/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939ms/step - accuracy: 0.9520 - loss: 0.1456\n",
      "Epoch 170: val_accuracy improved from 0.95120 to 0.95852, saving model to best_weights.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9519 - loss: 0.1457 - val_accuracy: 0.9585 - val_loss: 0.1345 - learning_rate: 0.0017\n",
      "Epoch 171/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9502 - loss: 0.1428\n",
      "Epoch 171: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9499 - loss: 0.1435 - val_accuracy: 0.9512 - val_loss: 0.1550 - learning_rate: 0.0017\n",
      "Epoch 172/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.9470 - loss: 0.1537\n",
      "Epoch 172: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9471 - loss: 0.1536 - val_accuracy: 0.9429 - val_loss: 0.1687 - learning_rate: 0.0017\n",
      "Epoch 173/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.9435 - loss: 0.1650\n",
      "Epoch 173: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9435 - loss: 0.1652 - val_accuracy: 0.9492 - val_loss: 0.1371 - learning_rate: 0.0017\n",
      "Epoch 174/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9412 - loss: 0.1729\n",
      "Epoch 174: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9414 - loss: 0.1725 - val_accuracy: 0.9502 - val_loss: 0.1486 - learning_rate: 0.0017\n",
      "Epoch 175/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9540 - loss: 0.1502\n",
      "Epoch 175: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9538 - loss: 0.1504 - val_accuracy: 0.9449 - val_loss: 0.1607 - learning_rate: 0.0017\n",
      "Epoch 176/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9498 - loss: 0.1489\n",
      "Epoch 176: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9497 - loss: 0.1491 - val_accuracy: 0.9405 - val_loss: 0.1755 - learning_rate: 0.0017\n",
      "Epoch 177/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869ms/step - accuracy: 0.9526 - loss: 0.1512\n",
      "Epoch 177: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 962ms/step - accuracy: 0.9525 - loss: 0.1513 - val_accuracy: 0.9405 - val_loss: 0.1762 - learning_rate: 0.0017\n",
      "Epoch 178/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908ms/step - accuracy: 0.9548 - loss: 0.1386\n",
      "Epoch 178: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 998ms/step - accuracy: 0.9548 - loss: 0.1386 - val_accuracy: 0.9478 - val_loss: 0.1553 - learning_rate: 0.0017\n",
      "Epoch 179/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841ms/step - accuracy: 0.9499 - loss: 0.1558\n",
      "Epoch 179: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 931ms/step - accuracy: 0.9499 - loss: 0.1558 - val_accuracy: 0.9400 - val_loss: 0.1757 - learning_rate: 0.0017\n",
      "Epoch 180/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804ms/step - accuracy: 0.9566 - loss: 0.1384\n",
      "Epoch 180: val_accuracy did not improve from 0.95852\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 0.0016290124622173607.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 892ms/step - accuracy: 0.9566 - loss: 0.1385 - val_accuracy: 0.9541 - val_loss: 0.1480 - learning_rate: 0.0017\n",
      "Epoch 181/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825ms/step - accuracy: 0.9498 - loss: 0.1569\n",
      "Epoch 181: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 913ms/step - accuracy: 0.9499 - loss: 0.1566 - val_accuracy: 0.9463 - val_loss: 0.1695 - learning_rate: 0.0016\n",
      "Epoch 182/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818ms/step - accuracy: 0.9558 - loss: 0.1343\n",
      "Epoch 182: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 904ms/step - accuracy: 0.9558 - loss: 0.1343 - val_accuracy: 0.9385 - val_loss: 0.1945 - learning_rate: 0.0016\n",
      "Epoch 183/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834ms/step - accuracy: 0.9554 - loss: 0.1428\n",
      "Epoch 183: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 924ms/step - accuracy: 0.9553 - loss: 0.1429 - val_accuracy: 0.9571 - val_loss: 0.1502 - learning_rate: 0.0016\n",
      "Epoch 184/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865ms/step - accuracy: 0.9547 - loss: 0.1358\n",
      "Epoch 184: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 967ms/step - accuracy: 0.9547 - loss: 0.1358 - val_accuracy: 0.9366 - val_loss: 0.2019 - learning_rate: 0.0016\n",
      "Epoch 185/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846ms/step - accuracy: 0.9548 - loss: 0.1389\n",
      "Epoch 185: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 935ms/step - accuracy: 0.9547 - loss: 0.1390 - val_accuracy: 0.9517 - val_loss: 0.1588 - learning_rate: 0.0016\n",
      "Epoch 186/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877ms/step - accuracy: 0.9494 - loss: 0.1455\n",
      "Epoch 186: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 966ms/step - accuracy: 0.9495 - loss: 0.1454 - val_accuracy: 0.9434 - val_loss: 0.1673 - learning_rate: 0.0016\n",
      "Epoch 187/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9590 - loss: 0.1344\n",
      "Epoch 187: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9589 - loss: 0.1344 - val_accuracy: 0.9458 - val_loss: 0.1652 - learning_rate: 0.0016\n",
      "Epoch 188/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910ms/step - accuracy: 0.9519 - loss: 0.1423\n",
      "Epoch 188: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9520 - loss: 0.1420 - val_accuracy: 0.9370 - val_loss: 0.1846 - learning_rate: 0.0016\n",
      "Epoch 189/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9574 - loss: 0.1340\n",
      "Epoch 189: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9574 - loss: 0.1339 - val_accuracy: 0.9453 - val_loss: 0.1651 - learning_rate: 0.0016\n",
      "Epoch 190/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978ms/step - accuracy: 0.9560 - loss: 0.1412 \n",
      "Epoch 190: val_accuracy did not improve from 0.95852\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 0.0015475617838092148.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9559 - loss: 0.1411 - val_accuracy: 0.9409 - val_loss: 0.1840 - learning_rate: 0.0016\n",
      "Epoch 191/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9572 - loss: 0.1332\n",
      "Epoch 191: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9571 - loss: 0.1333 - val_accuracy: 0.9434 - val_loss: 0.1794 - learning_rate: 0.0015\n",
      "Epoch 192/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9559 - loss: 0.1329\n",
      "Epoch 192: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9558 - loss: 0.1331 - val_accuracy: 0.9556 - val_loss: 0.1337 - learning_rate: 0.0015\n",
      "Epoch 193/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9611 - loss: 0.1164\n",
      "Epoch 193: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9612 - loss: 0.1166 - val_accuracy: 0.9468 - val_loss: 0.1741 - learning_rate: 0.0015\n",
      "Epoch 194/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9584 - loss: 0.1209\n",
      "Epoch 194: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9585 - loss: 0.1210 - val_accuracy: 0.9507 - val_loss: 0.1554 - learning_rate: 0.0015\n",
      "Epoch 195/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9542 - loss: 0.1275\n",
      "Epoch 195: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9543 - loss: 0.1273 - val_accuracy: 0.9439 - val_loss: 0.1965 - learning_rate: 0.0015\n",
      "Epoch 196/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9532 - loss: 0.1358\n",
      "Epoch 196: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9533 - loss: 0.1359 - val_accuracy: 0.9541 - val_loss: 0.1503 - learning_rate: 0.0015\n",
      "Epoch 197/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9527 - loss: 0.1389\n",
      "Epoch 197: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9528 - loss: 0.1388 - val_accuracy: 0.9453 - val_loss: 0.1716 - learning_rate: 0.0015\n",
      "Epoch 198/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.1307\n",
      "Epoch 198: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9567 - loss: 0.1304 - val_accuracy: 0.9473 - val_loss: 0.1627 - learning_rate: 0.0015\n",
      "Epoch 199/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9600 - loss: 0.1188\n",
      "Epoch 199: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9601 - loss: 0.1190 - val_accuracy: 0.9502 - val_loss: 0.1682 - learning_rate: 0.0015\n",
      "Epoch 200/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9601 - loss: 0.1221\n",
      "Epoch 200: val_accuracy did not improve from 0.95852\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9601 - loss: 0.1224 - val_accuracy: 0.9458 - val_loss: 0.1799 - learning_rate: 0.0015\n",
      "Restoring model weights from the end of the best epoch: 192.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_record = model.fit(x_train, y_train,\n",
    "                            epochs=200,\n",
    "                            batch_size=400,\n",
    "                            callbacks=[early_stopping, model_checkpoint, reduce_lr],  # ReduceLROnPlateau 추가\n",
    "                            validation_split=0.2,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 4407.0945773124695\n"
     ]
    }
   ],
   "source": [
    "end = time()\n",
    "print('time elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tensorflow adamw CNN-BILSTM_model0425.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(training_record.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(training_record.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "fig.suptitle(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 768, 1), dtype=float64, numpy=\n",
       "array([[[ 0.0448513 ],\n",
       "        [-0.04242964],\n",
       "        [-0.0069608 ],\n",
       "        ...,\n",
       "        [ 0.03132427],\n",
       "        [ 0.01451904],\n",
       "        [-0.03065181]],\n",
       "\n",
       "       [[ 0.05368535],\n",
       "        [-0.03171349],\n",
       "        [-0.00556636],\n",
       "        ...,\n",
       "        [ 0.01740213],\n",
       "        [ 0.0004463 ],\n",
       "        [-0.02583168]],\n",
       "\n",
       "       [[ 0.03783651],\n",
       "        [-0.03854481],\n",
       "        [-0.01468679],\n",
       "        ...,\n",
       "        [ 0.00171305],\n",
       "        [ 0.00638933],\n",
       "        [-0.02662061]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.04026417],\n",
       "        [-0.0731665 ],\n",
       "        [-0.02534198],\n",
       "        ...,\n",
       "        [ 0.01497611],\n",
       "        [ 0.01005374],\n",
       "        [-0.0048609 ]],\n",
       "\n",
       "       [[ 0.03292718],\n",
       "        [-0.01003936],\n",
       "        [-0.00947915],\n",
       "        ...,\n",
       "        [ 0.01867085],\n",
       "        [ 0.01455619],\n",
       "        [-0.02391131]],\n",
       "\n",
       "       [[ 0.0542101 ],\n",
       "        [-0.07168639],\n",
       "        [-0.0297229 ],\n",
       "        ...,\n",
       "        [ 0.00477823],\n",
       "        [ 0.02777079],\n",
       "        [-0.0124263 ]]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 2,\n",
       "       0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 2, 0, 0,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 0, 0, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0,\n",
       "       0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0,\n",
       "       1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 2, 2,\n",
       "       0, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "predicted_classes = np.argmax(pred, axis=1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=target.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0,\n",
       "       0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 2, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 0, 0, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0,\n",
       "       0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0,\n",
       "       1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "n_classes = 3  \n",
    "\n",
    "\n",
    "y_test_binarized = label_binarize(test_y, classes=[0, 1, 2])\n",
    "y_score= model.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "prc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    prc_auc[i] = auc(recall[i], precision[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='PRC curve of class {0} (area = {1:0.2f})'.format(i, prc_auc[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve for multi-class')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 2,\n",
       "       0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 2, 0, 0,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 0, 0, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0,\n",
       "       0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0,\n",
       "       1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 2, 2,\n",
       "       0, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0,\n",
       "       0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 2, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 0, 0, 2, 0, 2, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0,\n",
       "       0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 1, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0,\n",
       "       1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(-1)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_classes.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-LSTM 3 class Classifier의 정확도는: 0.9633\n"
     ]
    }
   ],
   "source": [
    "print('CNN-LSTM 3 class Classifier의 정확도는: {0:.4f}'.format(accuracy_score(test_y, predicted_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬:\n",
      "[[166   0   7]\n",
      " [  0  53   0]\n",
      " [  0   4  70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('혼동 행렬:')\n",
    "print(confusion_matrix(test_y, predicted_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNN LSTM confusion matrix 0425.txt\", \"w\") as text_file:\n",
    "    print(confusion_matrix(test_y, predicted_classes), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       173\n",
      "           1       0.93      1.00      0.96        53\n",
      "           2       0.91      0.95      0.93        74\n",
      "\n",
      "    accuracy                           0.96       300\n",
      "   macro avg       0.95      0.97      0.96       300\n",
      "weighted avg       0.97      0.96      0.96       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('분류 보고서:')\n",
    "report=classification_report(test_y, predicted_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNN-LSTM output 0425.txt\", \"w\") as text_file:\n",
    "    print(classification_report(test_y, predicted_classes,digits=4), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
